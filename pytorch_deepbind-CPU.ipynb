{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Research Example - DeepBind (2015)***\n",
    "# **Building the DeepBind model through a pytorch framework (CPU)**"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Preprocess**\n",
    "### 1.1. Load the raw data of ***ALX homeobox protein 1*** (SELEX sequencing data) \n",
    "- references \n",
    "    - http://tools.genes.toronto.edu/deepbind/D00289.002/index.html![image.png](attachment:image.png)\n",
    "    - https://github.com/MedChaabane/DeepBind-with-PyTorch\n",
    "- download \n",
    "    - [Training raw dataset](https://1drv.ms/u/s!AljTCh5CK9KvgYltlkAVHPv7kbdytg?e=qBtIgu) \n",
    "    - [Testing raw dataset](https://1drv.ms/u/s!AljTCh5CK9KvgYlsgGwcmTp5L-35iQ?e=k8eUxH) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Alx1_DBD_TAAAGC20NCG_3_Z_B.seq finish (255508 lines, #positive: 127754, #negative: 127754)\n",
      "- sequence sizes : {20}\n",
      "Load Alx1_DBD_TAAAGC20NCG_3_Z_A.seq finish (128012 lines, #positive: 128012, #negative: 0)\n",
      "- sequence sizes : {20}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def raw_seq_parser(path):\n",
    "    f = open(path, \"r\")\n",
    "    seqs = []\n",
    "    seq_lens = []\n",
    "    labels = []\n",
    "    flines = f.readlines()\n",
    "    cols = None\n",
    "    for i, line in enumerate(flines):\n",
    "        if not i :\n",
    "            cols = line.replace(\"\\n\",\"\").split(\"\\t\")\n",
    "        else:\n",
    "            fold_id, event_id, seq, bound = line.replace(\"\\n\",\"\").split(\"\\t\")\n",
    "            seqs.append(seq)\n",
    "            labels.append(int(bound))\n",
    "            seq_lens.append(len(seq))\n",
    "            \n",
    "    num_pos = sum(labels)\n",
    "    num_neg = len(labels) - num_pos\n",
    "    print(\"Load {} finish ({} lines, #positive: {}, #negative: {})\".format(path, len(labels), num_pos, num_neg))\n",
    "    print(\"- sequence sizes : {}\".format(set(seq_lens)))\n",
    "    \n",
    "    return seqs, labels\n",
    "\n",
    "\n",
    "# path of raw data\n",
    "TRAIN_RAW = \"Alx1_DBD_TAAAGC20NCG_3_Z_B.seq\"\n",
    "TEST_RAW  = \"Alx1_DBD_TAAAGC20NCG_3_Z_A.seq\"\n",
    "    \n",
    "train_seqs, y = raw_seq_parser(TRAIN_RAW)\n",
    "test_seqs, test_y = raw_seq_parser(TEST_RAW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. DNA sequence encoding + padding for motif detector + reverse-complement mode\n",
    "\n",
    "- Baseline: one-hot encoding\n",
    "- Due to the length of all sequences in the dataset is same (20bp), we don't need the encoding methods for variable length data\n",
    "- **Padding the letter 'N' in front of an input sequence and the end too**\n",
    "- When training a model on sequences derived from double-stranded DNA (PBM, SELEX, ChIP-seq), it is unknown whether the protein bound to the input strand or whether it bound to the opposite strand.\n",
    "\n",
    "#### *(Supplimentary information in DeepBind 2015)*\n",
    "<img src=\"figures/deepbind_supp_01.PNG\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sequence length 20\n",
      "One-hot encoding finished, with feature shape (511016, 38, 4)\n",
      "Original sequence length 20\n",
      "One-hot encoding finished, with feature shape (256024, 38, 4)\n"
     ]
    }
   ],
   "source": [
    "# ref [complement,reverse_complement] from [https://github.com/MedChaabane/DeepBind-with-PyTorch/]\n",
    "def complement(seq):\n",
    "    complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A', 'N': 'N'}\n",
    "    complseq = [complement[base] for base in seq]\n",
    "    return complseq\n",
    "  \n",
    "def reverse_complement(seq):\n",
    "    seq = list(seq)\n",
    "    seq.reverse()\n",
    "    return ''.join(complement(seq))\n",
    "\n",
    "def onehot_encoder_with_padding_and_reverse_complement(seqs, labels, letters=\"ACGT\", motif_len=10):\n",
    "    print(\"Original sequence length {}\".format(len(seqs[0])))\n",
    "    enc = defaultdict(lambda: np.array([1/len(letters)]*len(letters))) # default. [0.25,0.25,0.25,0.25]\n",
    "    for i,l in enumerate(letters):\n",
    "        f      = np.zeros(len(letters))\n",
    "        f[i]   = 1\n",
    "        enc[l] = f\n",
    "    \n",
    "    # reverse-complement (note that it have to apply for DNA seqs)\n",
    "    _seqs = []\n",
    "    _labels = []\n",
    "    for seq, label in zip(seqs, labels):\n",
    "        _seqs.append(seq)\n",
    "        _labels.append([label])\n",
    "        _seqs.append(reverse_complement(seq))\n",
    "        _labels.append([label])\n",
    "    \n",
    "    seqs = _seqs\n",
    "    labels = _labels\n",
    "    \n",
    "    # one-hot encoding\n",
    "    onehot_features = []\n",
    "    for seq in seqs:\n",
    "        # Append padding letter 'N' accoding to the <length of motif-1>\n",
    "        seq = 'N' * (motif_len-1) + seq + 'N' * (motif_len-1)\n",
    "        feature = []\n",
    "        for c in seq.upper():\n",
    "            feature.append(enc[c])\n",
    "        onehot_features.append(feature)\n",
    "        \n",
    "    onehot_features = np.array(onehot_features)\n",
    "    print(\"One-hot encoding finished, with feature shape {}\".format(onehot_features.shape))\n",
    "    return onehot_features, labels\n",
    "\n",
    "LEN_MOTIF      = 10\n",
    "X, y           = onehot_encoder_with_padding_and_reverse_complement(train_seqs, y, motif_len=LEN_MOTIF)\n",
    "test_x, test_y = onehot_encoder_with_padding_and_reverse_complement(test_seqs, test_y, motif_len=LEN_MOTIF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Split Train & Valid dataset \n",
    "- The testset has only positive class\n",
    "- Most evaluation metrices of binary classification problem need negative class\n",
    "- We split the training set to train and valid datasets\n",
    "- The testset only used when the model training is finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set | x: (342380, 38, 4), y: (342380, 1) \n",
      "Valid set | x: (168636, 38, 4), y: (168636, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = np.array(X), np.array(y) # convert python-list to numpy-array\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(X, y, \n",
    "                                                      test_size=0.33, # ratio of size of train-test set splitting\n",
    "                                                      random_state=42) # random-seed\n",
    "\n",
    "print(\"Train set | x: {}, y: {} \\nValid set | x: {}, y: {}\".format(train_x.shape,\n",
    "                                                                   train_y.shape,\n",
    "                                                                   valid_x.shape,\n",
    "                                                                   valid_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 4, 38) (500, 4, 38) (100, 4, 38)\n",
      "(5000, 1) (500, 1) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "train_x = train_x.reshape(-1, 4, 38)[:5000]\n",
    "valid_x = valid_x.reshape(-1, 4, 38)[:500]\n",
    "test_x  = test_x.reshape(-1, 4, 38)[:100]\n",
    "\n",
    "train_y = np.array(train_y[:5000])\n",
    "valid_y = np.array(valid_y[:500])\n",
    "test_y  = np.array(test_y[:100])\n",
    "\n",
    "print(train_x.shape, valid_x.shape, test_x.shape)\n",
    "print(train_y.shape, valid_y.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. 1D-Convolutional Neural Network for binary classification (pytorch)**\n",
    "### 2.1. Initialization of CNN-Model\n",
    "<img src=\"figures/deepbind_supp_02.PNG\" width=\"750\">\n",
    "\n",
    "#### *Convolution*\n",
    "- The defining feature of any ConvNet is that it begins with at least one convolution stage. \n",
    "- In the context of genomic sequences, a 1-dimensional convolution over a 4-channel input can play the same role as a ‚Äúmotif scan‚Äù operation in a PWM or PSAM-based model.\n",
    "\n",
    "#### *Rectification*\n",
    "- We call this a rectified motif scan.\n",
    "- In neural network terminology, this simple nonlinear operation is known as a rectified linear unit (ReLU) layer\n",
    "\n",
    "#### *Pooling*\n",
    "- We implemented two choices for the pooling stage: **‚Äúmax pooling‚Äù**, or **‚Äúmax pooling and average pooling.‚Äù**\n",
    "\n",
    "#### *Neural network*\n",
    "- We allow the choice of either **‚Äúno hidden layer‚Äù** or **‚Äúone hidden layer with 32 rectified-linear units‚Äù** ...\n",
    "- In this example, only **‚Äúno hidden layer‚Äù** will be implemented.\n",
    "- Do not cover the dropout part of DeepBind too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Motif scanners : torch.Size([16, 4, 10])\n",
      "- Input shape    : torch.Size([10, 4, 38])\n",
      "- After 1d-conv  : torch.Size([10, 16, 29])\n",
      "- After max-pool : torch.Size([10, 32])\n",
      "- After fc-net   : torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_motifs, len_motif, pooling_type=\"max\"): # pool_type = [max, maxavg]\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.l_conv = nn.Conv1d(in_channels  = 4, \n",
    "                                out_channels = num_motifs,\n",
    "                                kernel_size  = len_motif,\n",
    "                                stride       = 1, \n",
    "                                padding      = 0)\n",
    "        \n",
    "        self.pooling_type = pooling_type\n",
    "        self.relu = nn.ReLU()\n",
    "        if pooling_type == \"max\":\n",
    "            self.l_nnet = nn.Linear(num_motifs,1) \n",
    "        else:\n",
    "            self.l_nnet = nn.Linear(num_motifs*2,1)\n",
    "            \n",
    "        print(\"- Motif scanners : {}\".format(self.l_conv.weight.size()))\n",
    "        \n",
    "    def get_motif_scanners(self):\n",
    "        return self.l_conv.weight\n",
    "    \n",
    "    def get_rectified_motif_scan(self, x):\n",
    "        x = self.l_conv(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_test(self, x):\n",
    "        print(\"- Input shape    : {}\".format(x.size()))\n",
    "        x = self.relu(self.l_conv(x))\n",
    "        print(\"- After 1d-conv  : {}\".format(x.size()))\n",
    "        m , _ = torch.max(x,dim=2)\n",
    "        if self.pooling_type == \"maxavg\":\n",
    "            a = torch.mean(x, dim=2)\n",
    "            x = torch.cat((m,a), dim=1)\n",
    "        else:\n",
    "            x = m\n",
    "        print(\"- After max-pool : {}\".format(x.size()))\n",
    "        logit = self.l_nnet(x)\n",
    "        y = torch.sigmoid(logit)\n",
    "        print(\"- After fc-net   : {}\".format(y.size()))\n",
    "        return y\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.l_conv(x)\n",
    "        x = self.relu(x)\n",
    "        m , _ = torch.max(x,dim=2)\n",
    "        if self.pooling_type == \"maxavg\":\n",
    "            a = torch.mean(x, dim=2)\n",
    "            x = torch.cat((m,a), dim=1)\n",
    "        else:\n",
    "            x = m\n",
    "        logit = self.l_nnet(x)\n",
    "        y = torch.sigmoid(logit)\n",
    "        return y\n",
    "    \n",
    "NUM_MOTIFS = 16\n",
    "convnet = ConvNet(num_motifs   = NUM_MOTIFS, \n",
    "                  len_motif    = LEN_MOTIF,\n",
    "                  pooling_type = \"maxavg\")\n",
    "output  = convnet.forward_test(torch.randn(10, 4, 38))  # (num_seq, num_in_chn, len_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Initialization of model trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCELoss()\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn import datasets, metrics\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = optim.SGD(convnet.parameters(), lr=learning_rate)\n",
    "print(criterion)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 4, 38]) torch.Size([5000, 1])\n",
      "torch.Size([500, 4, 38]) torch.Size([500, 1])\n",
      "torch.Size([100, 4, 38]) torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "# numpy array to tensor\n",
    "ts_train_x = torch.tensor(train_x, dtype=torch.float32)\n",
    "ts_train_y = torch.tensor(train_y, dtype=torch.float32)\n",
    "ts_valid_x = torch.tensor(valid_x, dtype=torch.float32)\n",
    "ts_valid_y = torch.tensor(valid_y, dtype=torch.float32)\n",
    "ts_test_x  = torch.tensor(test_x, dtype=torch.float32)\n",
    "ts_test_y  = torch.tensor(test_y, dtype=torch.float32)\n",
    "print(ts_train_x.size(), ts_train_y.size())\n",
    "print(ts_valid_x.size(), ts_valid_y.size())\n",
    "print(ts_test_x.size(), ts_test_y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [(_x,_y) for _x, _y in zip(ts_train_x, ts_train_y)]\n",
    "valid_data = [(_x,_y) for _x, _y in zip(ts_valid_x, ts_valid_y)]\n",
    "test_data  = [(_x,_y) for _x, _y in zip(ts_test_x, ts_test_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset    = train_data, \n",
    "                                           batch_size = batch_size, \n",
    "                                           shuffle    = True)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(dataset    = valid_data,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle    = True)\n",
    "\n",
    "test_loader  = torch.utils.data.DataLoader(dataset    = test_data,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle    = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (l_conv): Conv1d(4, 16, kernel_size=(10,), stride=(1,))\n",
       "  (relu): ReLU()\n",
       "  (l_nnet): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch      0 | Train Loss 0.69392 | Valid Loss 0.69337 | Valid ROC-AUC 0.50537\n",
      "Epoch      1 | Train Loss 0.69226 | Valid Loss 0.69138 | Valid ROC-AUC 0.54976\n",
      "Epoch      2 | Train Loss 0.69071 | Valid Loss 0.69070 | Valid ROC-AUC 0.58764\n",
      "Epoch      3 | Train Loss 0.68883 | Valid Loss 0.68889 | Valid ROC-AUC 0.61236\n",
      "Epoch      4 | Train Loss 0.68699 | Valid Loss 0.68731 | Valid ROC-AUC 0.63089\n",
      "Epoch      5 | Train Loss 0.68463 | Valid Loss 0.68554 | Valid ROC-AUC 0.64320\n",
      "Epoch      6 | Train Loss 0.68193 | Valid Loss 0.68327 | Valid ROC-AUC 0.65236\n",
      "Epoch      7 | Train Loss 0.67881 | Valid Loss 0.67981 | Valid ROC-AUC 0.65680\n",
      "Epoch      8 | Train Loss 0.67521 | Valid Loss 0.67756 | Valid ROC-AUC 0.66233\n",
      "Epoch      9 | Train Loss 0.67164 | Valid Loss 0.67405 | Valid ROC-AUC 0.66734\n",
      "Epoch     10 | Train Loss 0.66718 | Valid Loss 0.67066 | Valid ROC-AUC 0.66965\n",
      "Epoch     11 | Train Loss 0.66299 | Valid Loss 0.66612 | Valid ROC-AUC 0.67304\n",
      "Epoch     12 | Train Loss 0.65843 | Valid Loss 0.66281 | Valid ROC-AUC 0.67482\n",
      "Epoch     13 | Train Loss 0.65374 | Valid Loss 0.65909 | Valid ROC-AUC 0.67738\n",
      "Epoch     14 | Train Loss 0.64909 | Valid Loss 0.65533 | Valid ROC-AUC 0.67962\n",
      "Epoch     15 | Train Loss 0.64447 | Valid Loss 0.65060 | Valid ROC-AUC 0.68180\n",
      "Epoch     16 | Train Loss 0.63900 | Valid Loss 0.64701 | Valid ROC-AUC 0.68396\n",
      "Epoch     17 | Train Loss 0.63465 | Valid Loss 0.64269 | Valid ROC-AUC 0.68674\n",
      "Epoch     18 | Train Loss 0.63088 | Valid Loss 0.63917 | Valid ROC-AUC 0.68907\n",
      "Epoch     19 | Train Loss 0.62622 | Valid Loss 0.63560 | Valid ROC-AUC 0.69180\n",
      "Epoch     20 | Train Loss 0.62162 | Valid Loss 0.63179 | Valid ROC-AUC 0.69484\n",
      "Epoch     21 | Train Loss 0.61745 | Valid Loss 0.62878 | Valid ROC-AUC 0.69767\n",
      "Epoch     22 | Train Loss 0.61402 | Valid Loss 0.62504 | Valid ROC-AUC 0.70049\n",
      "Epoch     23 | Train Loss 0.60964 | Valid Loss 0.62202 | Valid ROC-AUC 0.70331\n",
      "Epoch     24 | Train Loss 0.60559 | Valid Loss 0.61950 | Valid ROC-AUC 0.70639\n",
      "Epoch     25 | Train Loss 0.60376 | Valid Loss 0.61564 | Valid ROC-AUC 0.70999\n",
      "Epoch     26 | Train Loss 0.60018 | Valid Loss 0.61310 | Valid ROC-AUC 0.71339\n",
      "Epoch     27 | Train Loss 0.59719 | Valid Loss 0.61088 | Valid ROC-AUC 0.71726\n",
      "Epoch     28 | Train Loss 0.59428 | Valid Loss 0.60679 | Valid ROC-AUC 0.72200\n",
      "Epoch     29 | Train Loss 0.59052 | Valid Loss 0.60360 | Valid ROC-AUC 0.72623\n",
      "Epoch     30 | Train Loss 0.58658 | Valid Loss 0.60049 | Valid ROC-AUC 0.73035\n",
      "Epoch     31 | Train Loss 0.58475 | Valid Loss 0.59728 | Valid ROC-AUC 0.73454\n",
      "Epoch     32 | Train Loss 0.58175 | Valid Loss 0.59454 | Valid ROC-AUC 0.73824\n",
      "Epoch     33 | Train Loss 0.57766 | Valid Loss 0.59111 | Valid ROC-AUC 0.74282\n",
      "Epoch     34 | Train Loss 0.57536 | Valid Loss 0.58894 | Valid ROC-AUC 0.74668\n",
      "Epoch     35 | Train Loss 0.57168 | Valid Loss 0.58460 | Valid ROC-AUC 0.75227\n",
      "Epoch     36 | Train Loss 0.56818 | Valid Loss 0.58318 | Valid ROC-AUC 0.75588\n",
      "Epoch     37 | Train Loss 0.56634 | Valid Loss 0.57868 | Valid ROC-AUC 0.76014\n",
      "Epoch     38 | Train Loss 0.56246 | Valid Loss 0.57556 | Valid ROC-AUC 0.76427\n",
      "Epoch     39 | Train Loss 0.55948 | Valid Loss 0.57763 | Valid ROC-AUC 0.76725\n",
      "Epoch     40 | Train Loss 0.55658 | Valid Loss 0.57023 | Valid ROC-AUC 0.77186\n",
      "Epoch     41 | Train Loss 0.55447 | Valid Loss 0.56818 | Valid ROC-AUC 0.77433\n",
      "Epoch     42 | Train Loss 0.55128 | Valid Loss 0.56529 | Valid ROC-AUC 0.77777\n",
      "Epoch     43 | Train Loss 0.54784 | Valid Loss 0.56215 | Valid ROC-AUC 0.78165\n",
      "Epoch     44 | Train Loss 0.54443 | Valid Loss 0.55953 | Valid ROC-AUC 0.78520\n",
      "Epoch     45 | Train Loss 0.54280 | Valid Loss 0.55726 | Valid ROC-AUC 0.78725\n",
      "Epoch     46 | Train Loss 0.53996 | Valid Loss 0.55503 | Valid ROC-AUC 0.78985\n",
      "Epoch     47 | Train Loss 0.53708 | Valid Loss 0.55400 | Valid ROC-AUC 0.79183\n",
      "Epoch     48 | Train Loss 0.53528 | Valid Loss 0.55061 | Valid ROC-AUC 0.79478\n",
      "Epoch     49 | Train Loss 0.53320 | Valid Loss 0.55282 | Valid ROC-AUC 0.79677\n"
     ]
    }
   ],
   "source": [
    "convnet.train()\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    train_loss = []\n",
    "    for index, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = convnet(data)\n",
    "        loss   = criterion(output, target)\n",
    "        loss.backward()  \n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "    \n",
    "    # Validation\n",
    "    valid_output = convnet(ts_valid_x.to(device))\n",
    "    valid_loss = criterion(valid_output.to(device), ts_valid_y.to(device))\n",
    "    valid_loss = valid_loss.item()\n",
    "    valid_roc_auc = metrics.roc_auc_score(ts_valid_y.detach().numpy(),\n",
    "                                          valid_output.detach().numpy())\n",
    "    \n",
    "    print(\"Epoch {:>6} | Train Loss {:.5f} | Valid Loss {:.5f} | Valid ROC-AUC {:.5f}\".format(epoch, \n",
    "                                                                                              np.mean(train_loss),\n",
    "                                                                                              valid_loss, \n",
    "                                                                                              valid_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.71      0.83       100\n",
      "\n",
      "    accuracy                           0.71       100\n",
      "   macro avg       0.50      0.35      0.42       100\n",
      "weighted avg       1.00      0.71      0.83       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_output = convnet(ts_test_x.to(device)).detach().numpy()\n",
    "test_output = test_output > 0.5\n",
    "print(metrics.classification_report(test_y, test_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Visualizations for DeepBind models**\n",
    "### 3.1. Mutation Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Sequence Logos\n",
    "To visualize the pattern learned by a particular DeepBind motif detector $ùëÄ_ùëò$, we generate a PWM derived from the detector‚Äôs response to actual sequences.\n",
    "\n",
    "1. We feed all sequences from the test set (foreground and background) through the convolutional and rectification stages of the DeepBind model\n",
    "2. We align all the sequences that passed the activation threshold\n",
    "3. Once the sequences are aligned, we generate a position frequency matrix (PFM)\n",
    "4. Transform it into a sequence logo in the standard way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 ConvNet(\n",
      "  (l_conv): Conv1d(4, 16, kernel_size=(10,), stride=(1,))\n",
      "  (relu): ReLU()\n",
      "  (l_nnet): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "Test input sequences for extracting motif torch.Size([100, 4, 38])\n",
      "Returned rectified motif scan data (100, 16, 29)\n"
     ]
    }
   ],
   "source": [
    "# < Position Frequency Matrix using DeepBind model >\n",
    "# \n",
    "# 1. Get rectified motif scan result (from model) from the test dataset sequences\n",
    "# 2. For each motif, find maximum valued position (j)\n",
    "# 3. Extract subsequence from input sequence where (j-m+1) ~ (j)\n",
    "# 3-1 when the positions (j-m+1) ~ (j) beyond the sequence range, fill the empty (letter N)\n",
    "# 4. Accumulate all subsequences (length m) and count letter of each position\n",
    "\n",
    "def get_subsequence(seq_x, start, end, letters=\"ACGT\"):\n",
    "    ret = \"\"\n",
    "    ldict = {i:l for i,l in enumerate(letters)}\n",
    "    if start < 0:\n",
    "        ret = \"N\"*(-start) + ret\n",
    "        for i in range(end):\n",
    "            x = seq_x[:, i]\n",
    "            base = ldict[np.argmax(x)]\n",
    "            ret = ret + base\n",
    "    else:\n",
    "        for i in range(start, end):\n",
    "            x = seq_x[:, i]\n",
    "            base = ldict[np.argmax(x)]\n",
    "            ret = ret + base\n",
    "    \n",
    "    return ret\n",
    "\n",
    "# Check the global parameters\n",
    "print(LEN_MOTIF, convnet)\n",
    "print(\"Test input sequences for extracting motif {}\".format(ts_test_x.shape))\n",
    "\n",
    "# 1. Get rectified motif scan result (from model) from the test dataset sequences\n",
    "rect_scans = convnet.get_rectified_motif_scan(ts_test_x.to(device)).detach().numpy() \n",
    "print(\"Returned rectified motif scan data {}\".format(rect_scans.shape))\n",
    "\n",
    "# 4. Accumulate all subsequences (length m) and count letter of each position\n",
    "test_subseqs_list = defaultdict(lambda: [])\n",
    "\n",
    "for test_seq, rect_scan in zip(ts_test_x, rect_scans):    \n",
    "    test_seq = test_seq.detach().numpy()\n",
    "    \n",
    "    for m_i, m_scans in enumerate(rect_scan):\n",
    "        # 2. For each motif, find maximum valued position (j)\n",
    "        j = np.argmax(m_scans)\n",
    "        j_m_1 = j - LEN_MOTIF + 1\n",
    "        \n",
    "        # 3. Extract subsequence from input sequence where (j-m+1) ~ (j)\n",
    "        # 3-1 when the positions (j-m+1) ~ (j) beyond the sequence range, fill the empty (letter N)\n",
    "        subseq = get_subsequence(test_seq, j_m_1, j+1)\n",
    "        \n",
    "        # print(m_i+1, j_m_1, j, subseq, len(subseq))\n",
    "        test_subseqs_list[m_i].append(subseq)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_subseqs_list.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CAACGACAGA',\n",
       " 'CAAGCAAGGC',\n",
       " 'GCAAAAGCGA',\n",
       " 'AAGAACGCGA',\n",
       " 'NNNNNNGCAC',\n",
       " 'CAAAGACAAC',\n",
       " 'AGGCACGAAC',\n",
       " 'NNNNNNNNNA',\n",
       " 'NNNNNNNNNC',\n",
       " 'NNNNAAGACA',\n",
       " 'NNGCACAAAC',\n",
       " 'NNNNACAAGC',\n",
       " 'AACACAGACA',\n",
       " 'NNNNNNCAAA',\n",
       " 'AAACAAAGGC',\n",
       " 'AAACAAAGAA',\n",
       " 'NNNNNNCACA',\n",
       " 'NNNNNNNNNC',\n",
       " 'NNGACAGAAC',\n",
       " 'AGCAACGAAC',\n",
       " 'AGAAACAGGC',\n",
       " 'NNNNNNCGAC',\n",
       " 'NNCAACGAAA',\n",
       " 'AAACAGGCAC',\n",
       " 'NNNNNNNNAT',\n",
       " 'CGAAAGGCCA',\n",
       " 'NNNNNNCAAC',\n",
       " 'AGAAAACGAA',\n",
       " 'NNNNATAGGC',\n",
       " 'NNTAGCGAAC',\n",
       " 'ACAAGAGCAA',\n",
       " 'CCAAACGGAA',\n",
       " 'GCCAAGAAAC',\n",
       " 'CAAACGAACA',\n",
       " 'ACAAACAACA',\n",
       " 'AGGCAAGCCA',\n",
       " 'NNNNNNNNGC',\n",
       " 'AAACCGGAAA',\n",
       " 'NNNNTCAGGC',\n",
       " 'AGGCAACAAC',\n",
       " 'ACACGAAAGC',\n",
       " 'GAACAAGAAG',\n",
       " 'CCAGAAAACA',\n",
       " 'NNNNNNNACA',\n",
       " 'NNCAACGAAA',\n",
       " 'AAAGCAAAGC',\n",
       " 'AAGCAACAAA',\n",
       " 'ACAAGCAACA',\n",
       " 'ACAAGCGAAC',\n",
       " 'CCAGACAAAG',\n",
       " 'CAAGCCAGAA',\n",
       " 'CAGGCAACAA',\n",
       " 'CGGACAAGCA',\n",
       " 'CAGACAAGGC',\n",
       " 'CCAGACAAGA',\n",
       " 'CAGCAAGCGA',\n",
       " 'AAACAAAGGC',\n",
       " 'ACAAGACAAC',\n",
       " 'NNGCAACAAA',\n",
       " 'GAACCGAAGC',\n",
       " 'NNNNNNGCCA',\n",
       " 'NNGCACGACA',\n",
       " 'NNNNNNGACA',\n",
       " 'AACGAAACAA',\n",
       " 'GCAAGAAACA',\n",
       " 'CCGAAGAACA',\n",
       " 'NNNNNNNNGC',\n",
       " 'CAACAAGACA',\n",
       " 'GCGAAAGCCA',\n",
       " 'ACGAAAGGCA',\n",
       " 'GAGCCAAAGC',\n",
       " 'CAAGCCAAGC',\n",
       " 'AAAAGACACA',\n",
       " 'AAACACAGAC',\n",
       " 'GCACGAAACA',\n",
       " 'NNGAAACGAA',\n",
       " 'CCAGACAGAA',\n",
       " 'CGAAGACAAA',\n",
       " 'NNNNNNNNCT',\n",
       " 'AGCAAAGCAA',\n",
       " 'GCACGAAAGC',\n",
       " 'NNNNNNNNGT',\n",
       " 'GCAACAGACA',\n",
       " 'ACAAAGGCCA',\n",
       " 'CGAAGAACGC',\n",
       " 'NNNNNNGAAC',\n",
       " 'ACAGGCAAGC',\n",
       " 'NNNNNNNNCA',\n",
       " 'NNNNNNGTAC',\n",
       " 'AACACAGAAC',\n",
       " 'ACGAACGAAC',\n",
       " 'NNNNNNNNGC',\n",
       " 'NNNNNNNGCA',\n",
       " 'GAAACAGACA',\n",
       " 'CCAGACAAGA',\n",
       " 'NNNNNNNGCA',\n",
       " 'CCAGACAAGC',\n",
       " 'NNNNACAGAC',\n",
       " 'AAACCGGAAC',\n",
       " 'GACAAAAGGC']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_subseqs_list[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
